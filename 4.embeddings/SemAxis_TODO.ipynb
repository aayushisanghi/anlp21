{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SemAxis](https://arxiv.org/pdf/1806.05521.pdf) is a method for scoring terms along a user-defined axis (e.g., positive-negative, concrete-abstract, hot-cold), which can be used for a range of empirical questions (for one example, see [Kozlowski et al. 2019](https://journals.sagepub.com/doi/full/10.1177/0003122419877135)). In this homework, you'll implement SemAxis using word representations from Glove, and use it to explore corpus-specific conceptual associations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running, install gensim with:\n",
    "\n",
    "`conda install gensim`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we'll be working with pre-trained word embeddings using the `gensim` library, which provides a number of functions for accessing representations for individual words and comparing them.  The representations we'll use come from [Glove](https://nlp.stanford.edu/projects/glove/), which are trained on web data from the [Common Crawl](https://en.wikipedia.org/wiki/Common_Crawl) corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/f5_sk7l16f1cr1htyq1k3c5c0000gn/T/ipykernel_25144/2636338883.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, glove_in_w2v_format)\n"
     ]
    }
   ],
   "source": [
    "# First we have to convert the Glove format into w2v format; this creates a new file\n",
    "glove_file=\"../data/glove.6B.100d.100K.txt\"\n",
    "glove_in_w2v_format=\"../data/glove.6B.100d.100K.w2v.txt\"\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(\"../data/glove.6B.100d.100K.w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_vector=glove[\"good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.030769   0.11993    0.53909   -0.43696   -0.73937   -0.15345\n",
      "  0.081126  -0.38559   -0.68797   -0.41632   -0.13183   -0.24922\n",
      "  0.441      0.085919   0.20871   -0.063582   0.062228  -0.051234\n",
      " -0.13398    1.1418     0.036526   0.49029   -0.24567   -0.412\n",
      "  0.12349    0.41336   -0.48397   -0.54243   -0.27787   -0.26015\n",
      " -0.38485    0.78656    0.1023    -0.20712    0.40751    0.32026\n",
      " -0.51052    0.48362   -0.0099498 -0.38685    0.034975  -0.167\n",
      "  0.4237    -0.54164   -0.30323   -0.36983    0.082836  -0.52538\n",
      " -0.064531  -1.398     -0.14873   -0.35327   -0.1118     1.0912\n",
      "  0.095864  -2.8129     0.45238    0.46213    1.6012    -0.20837\n",
      " -0.27377    0.71197   -1.0754    -0.046974   0.67479   -0.065839\n",
      "  0.75824    0.39405    0.15507   -0.64719    0.32796   -0.031748\n",
      "  0.52899   -0.43886    0.67405    0.42136   -0.11981   -0.21777\n",
      " -0.29756   -0.1351     0.59898    0.46529   -0.58258   -0.02323\n",
      " -1.5442     0.01901   -0.015877   0.024499  -0.58017   -0.67659\n",
      " -0.040379  -0.44043    0.083292   0.20035   -0.75499    0.16918\n",
      " -0.26573   -0.52878    0.17584    1.065    ]\n"
     ]
    }
   ],
   "source": [
    "print(good_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions useful for the first question include the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
      " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
      " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
      " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
      "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
      " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
      " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
      "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
      "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
      "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
      " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
      "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
      "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
      " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
      " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
      " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
      " -0.69183   -1.0426     0.28855    0.63056  ]\n",
      "(100,) (100,) (100,) [0.7592798]\n"
     ]
    }
   ],
   "source": [
    "# access the representation for a single word\n",
    "great_vector=glove[\"great\"]\n",
    "print(great_vector)\n",
    "# use numpy to average multiple vector representations together\n",
    "vecs_to_average=[good_vector, great_vector]\n",
    "average=np.mean(vecs_to_average, axis=0)\n",
    "\n",
    "# calculate the cosine similariy between two vectors\n",
    "cosine_similarity=glove.cosine_similarities(good_vector, [great_vector])\n",
    "\n",
    "print(good_vector.shape, great_vector.shape, average.shape, cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Read the SemAxis [paper](https://arxiv.org/pdf/1806.05521.pdf) and implement the SemAxis method described in sections 3.1.2 and 3.1.3.  Given a set of word embeddings for positive terms $S^+ = \\{v_1^+, \\ldots v_n^+\\}$ and embeddings for negative terms $S^- = \\{v_1^-, \\ldots v_n^-\\}$ that define the endpoints of the axis, your output should be a single real-value score for an input word $w$ with word representation $v_w$:\n",
    "\n",
    "$$\n",
    "score(w)_{\\mathbf{V_\\textrm{axis}}} = \\textrm{cos}(v_w, \\mathbf{V}_\\textrm{axis})\n",
    "$$\n",
    "\n",
    "Where: \n",
    "$$\n",
    "\\mathbf{V}^+ = {1 \\over n} \\sum_1^n v_i^+\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}^- = {1 \\over m} \\sum_1^m v_i^-\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\textrm{axis}} = \\mathbf{V}^+ - \\mathbf{V}^-\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semaxis_score(vectors, positive_terms=None, negative_terms=None, target_word=None):\n",
    "    \n",
    "    # your code here\n",
    "    positive_vecs=[]\n",
    "    negative_vecs=[]\n",
    "    for term in positive_terms:\n",
    "        positive_vecs.append(glove[term])\n",
    "    for term in negative_terms:\n",
    "        negative_vecs.append(glove[term])\n",
    "    pos_average=np.mean(positive_vecs, axis=0)\n",
    "    neg_average=np.mean(negative_vecs, axis=0)\n",
    "    v_axis = pos_average - neg_average\n",
    "    # score should be a single real-value number (e.g., 0.342)\n",
    "    score = glove.cosine_similarities(glove[target_word], [v_axis])\n",
    "    return score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3424988"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be 0.342\n",
    "get_semaxis_score(glove, positive_terms=[\"woman\", \"women\"], negative_terms=[\"man\", \"men\"], target_word=\"actress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's score a set of target terms along that axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_list_of_targets(vectors, positive_terms=None, negative_terms=None, target_words=None):\n",
    "    scores=[]\n",
    "    for target in target_words:\n",
    "        scores.append((get_semaxis_score(vectors, positive_terms, negative_terms, target), target))\n",
    "\n",
    "    for k,v in reversed(sorted(scores)):\n",
    "        print(\"%.3f\\t%s\" % (k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=[\"doctor\", \"nurse\", \"actor\", \"actress\", \"mechanic\", \"librarian\", \"architect\", \"magician\", \"cook\", \"chef\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342\tactress\n",
      "0.294\tnurse\n",
      "0.219\tlibrarian\n",
      "0.106\tdoctor\n",
      "0.024\tactor\n",
      "0.003\tchef\n",
      "-0.019\tcook\n",
      "-0.075\tarchitect\n",
      "-0.153\tmagician\n",
      "-0.194\tmechanic\n"
     ]
    }
   ],
   "source": [
    "score_list_of_targets(glove, positive_terms=[\"woman\", \"women\"], negative_terms=[\"man\", \"men\"], target_words=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Define your own concept axis by selecting a set of positive and negative terms and illustrate its utility by scoring a set of 10 target terms (as we did above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.147\tprofessor\n",
      "0.129\tteacher\n",
      "0.123\tnurse\n",
      "0.096\tchef\n",
      "0.053\tfemale\n",
      "0.038\tmale\n",
      "0.010\tdoctor\n",
      "-0.070\tengineer\n",
      "-0.161\tcops\n",
      "-0.254\tpolice\n"
     ]
    }
   ],
   "source": [
    "positive_terms=['gentle']\n",
    "negative_terms=['rough']\n",
    "targets=['male','female','doctor','nurse','chef','engineer','teacher','professor','police','cops']\n",
    "\n",
    "score_list_of_targets(glove, positive_terms=positive_terms, negative_terms=negative_terms, target_words=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.175\thandsome\n",
      "0.153\tugly\n",
      "0.104\tbeautiful\n",
      "0.088\tactor\n",
      "0.048\tpretty\n",
      "0.047\tactress\n",
      "0.026\tunattractive\n",
      "-0.045\tattractive\n",
      "-0.109\tundesirable\n",
      "-0.140\tmodel\n",
      "-0.211\tdesirable\n"
     ]
    }
   ],
   "source": [
    "positive_terms=['brown','dark']\n",
    "negative_terms=['white','fair']\n",
    "targets=['beautiful','ugly','pretty','handsome','attractive','unattractive','model','desirable','undesirable','actor','actress']\n",
    "score_list_of_targets(glove, positive_terms=positive_terms, negative_terms=negative_terms, target_words=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Let's assume now that you're able to score all words in a vocabulary along several conceptual dimensions (like the one you've defined) for a given set of word embeddings trained on a dataset.  What could you do with that score? Brainstorm possible applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the model is trained on a specialised dataset like tweets from supporters of 2 political candidates, it can be used to understand their stance on key issues. \n",
    "2. Similarly, this can be applied to any scenario where we want to evaluate the stance of a community on an issue, even if it is as simple as product reviews, and how they span across different consumer groups.\n",
    "3. It can help to understand societal perceptions like gender roles, beauty standards, etc. (As shown above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
