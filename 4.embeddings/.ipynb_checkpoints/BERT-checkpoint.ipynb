{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5792cb96",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore the basics of token representations in BERT and use it to find token nearest neighbors.  Before using, be sure to install the following libraries:\n",
    "\n",
    "```sh\n",
    "conda install -c conda-forge ipywidgets\n",
    "conda install -c huggingface transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddc388c",
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "The 'sacremoses' distribution was not found and is required by this application. \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mgot_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/importlib/metadata.py\u001b[0m in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[0;32m--> 530\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/importlib/metadata.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \"\"\"\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistribution_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/importlib/metadata.py\u001b[0m in \u001b[0;36mfrom_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: sacremoses",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b2/f5_sk7l16f1cr1htyq1k3c5c0000gn/T/ipykernel_34292/3292001544.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .file_utils import (\n\u001b[1;32m     45\u001b[0m     \u001b[0m_LazyModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;34m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mgot_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPackageNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         raise importlib_metadata.PackageNotFoundError(\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;34mf\"The '{requirement}' distribution was not found and is required by this application. {hint}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         )\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: The 'sacremoses' distribution was not found and is required by this application. \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a12638",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f34541",
   "metadata": {},
   "source": [
    "BERT uses WordPiece tokenization, which breaks down words that don't appear within its 30K-word vocabulary into small pieces.  The word \"vaccinated\", for instanced, is tokenized as [\"va\", \"##cci\", \"##nated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"New data shows 26 states have fully vaccinated more than half their residents.\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"BERT is supercalifragilisticexpialidocious\", return_tensors=\"pt\")\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058b2be",
   "metadata": {},
   "source": [
    "As mentioned in class, notice how every sentence input to BERT is wrapped in two tags: a start [CLS] tag and an ending [SEP] tag.  BERT will generate representations of each WordPiece token, including these special [CLS] and [SEP] tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9c2fa",
   "metadata": {},
   "source": [
    "To generate representations for the input tokens, simply pass the input through the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=tokenizer(\"This jam is delicious\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3f309",
   "metadata": {},
   "source": [
    "Representations for each of BERT layers (12 in this model) are accessible here, but let's explore just the outputs from the final layer.  This BERT model has 768-dimensional representations, so this 6-token input ([CLS, this, jam, is, delicious, [SEP]) has an output that is is a 1 x 6 tokens x 768 dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe68cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888e87d",
   "metadata": {},
   "source": [
    "We'll cover using BERT for supervised problems in later classes, but what can we do with just these representations?  While we used word2vec-style static embeddings to find nearest neighbors for word *types*, we can do the same here for word *tokens*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"I ate some jam with toast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sents=[\"She got me out of a real jam\", \"This jam is made of strawberries\", \"I sat in a traffic jam for 2 hours\", \"The Grateful Dead used to jam for like two days straight.\", \"My grandma makes the best jam.\", \"I had to jam on the brakes to avoid hitting him.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be898d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_for_token(string, term):\n",
    "    \n",
    "    # tokenize\n",
    "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "    \n",
    "    # convert input ids to words\n",
    "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    \n",
    "    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n",
    "    term_idx=tokens.index(term)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # return the BERT rep for that token index\n",
    "    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n",
    "    \n",
    "    return outputs.last_hidden_state[0][term_idx].detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719345c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rep=get_bert_for_token(query, \"jam\")\n",
    "print(query_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0e1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals=[]\n",
    "for sent in comp_sents:\n",
    "    comp_rep=get_bert_for_token(sent, \"jam\")\n",
    "    cos_sim=cosine_similarity(query_rep, comp_rep)\n",
    "    vals.append((cos_sim, query, sent))\n",
    "\n",
    "for c, q, s in reversed(sorted(vals)):\n",
    "    print(\"%.3f\\t%s\\t%s\" % (c, q, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7167cc",
   "metadata": {},
   "source": [
    "**Q**: Brainstorm the variety of things you can do with token-level word representations like this and we'll discuss them at the end of class.  As one example, adapt the code above to find *any* word that is most similar to *jam* in \"I ate some jam with toast\" in the following sentences.  Are you able to find token-level synonyms this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sents=[\"My grandma makes the best jelly.\", \"Jelly is made of strawberries\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42395734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
