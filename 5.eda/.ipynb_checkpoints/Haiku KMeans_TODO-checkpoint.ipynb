{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8780af9",
   "metadata": {},
   "source": [
    "In \"[Literary Pattern Recognition](https://www.journals.uchicago.edu/doi/full/10.1086/684353)\", Long and So train a classifier to differentiate haiku poems from non-haiku poems, and find that many features help do so.  In class, we've discussed the importance of representation--how you *describe* a text computationally influences the kinds of things you are able to do with it.  While Long and So explore description in the context of classification, in this homework, you'll see how well you can design features that can differentiate these two classes *without* any supervision. Are you able to featurize a collection of poems such that two clusters (haiku/non-haiku) emerge when using KMeans clustering, with the text representation as your only degree of freedom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30ae833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, re\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f517024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(path, metadata, filepath_col):\n",
    "    data=[]\n",
    "    with open(metadata, encoding=\"utf-8\") as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader)\n",
    "        for cols in csv_reader:\n",
    "            poem_path=os.path.join(path, cols[filepath_col])\n",
    "            if os.path.exists(poem_path):\n",
    "                with open(poem_path, encoding=\"utf-8\") as poem_file:\n",
    "                    poem=poem_file.read()\n",
    "                    data.append(poem)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81026d2",
   "metadata": {},
   "source": [
    "Here we'll use data originally released on Github to support \"Literary Pattern Recognition\": [https://github.com/hoytlong/PatternRecognition](https://github.com/hoytlong/PatternRecognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbe9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku=read_texts(\"../data/haiku/long_so_haiku\", \"../data/haiku/Haikus.csv\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336544fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "others=read_texts(\"../data/haiku/long_so_others\", \"../data/haiku/OthersData.csv\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b4d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change anything within this code block\n",
    "\n",
    "def run_all(haiku, others, feature_function):\n",
    "    \n",
    "    X, Y, featurize_vocab=feature_function(haiku, others)\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "    nmi=metrics.normalized_mutual_info_score(Y, kmeans.labels_)\n",
    "    print(\"%.3f NMI\" % nmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c16b1",
   "metadata": {},
   "source": [
    "As one example, let's take a simple featurization and represent each poem by a binary indicator of the dictionary word types it contains.  \"To be or not to be\", for example, would be represented as {\"to\": 1, \"be\": 1, \"or\": 1, \"not\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f433360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a list of haiku poems and non-haiku poems, and returns:\n",
    "\n",
    "# X (sparse matrix, with poems as rows and features as columns)\n",
    "# Y (list of poem labels, with 1=haiku and 0=non-haiku)\n",
    "# feature_vocab (dict mapping feature name to feature ID)\n",
    "\n",
    "def unigram_featurize_all(haiku, others):\n",
    "\n",
    "    def unigram_featurize(poem, feature_vocab):\n",
    "        \n",
    "        # featurize text by just noting the binary presence of words within it\n",
    "        \n",
    "        feats={}\n",
    "\n",
    "        tokens=nltk.word_tokenize(poem.lower())\n",
    "        for token in tokens:\n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token]=len(feature_vocab)\n",
    "            feats[feature_vocab[token]]=1\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=unigram_featurize(poem, feature_vocab)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f]\n",
    "    # print(X)\n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695d6fb",
   "metadata": {},
   "source": [
    "This method yields an NMI of ~0.07 (with some variability due to the randomness of KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b420ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.073 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, unigram_featurize_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e57de6",
   "metadata": {},
   "source": [
    "**Q1**: Copy the `unigram_featurize_all` code above and adapt it to create your own featurization method named `fancy_featurize_all`.  You may use whatever information you like to represent these poems for the purposes of clustering them into two categories, but you must use the KMeans clustering (with 2 clusters) as defined in `run_all`.  Use your own understanding of haiku, or read the Long and So article above for other ideas.  Are you able to improve over an NMI of 0.07?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19b47e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# approach 1: using TF-IDF word vectors\n",
    "def fancy_featurize_all_1(haiku, others):\n",
    "    \n",
    "    def tfIdf_featurize(poem, feature_vocab,matrix):\n",
    "        \n",
    "        # featurize text using tf-idf\n",
    "        \n",
    "        feats={}\n",
    "\n",
    "        tokens=nltk.word_tokenize(poem)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token not in feature_vocab:\n",
    "                feature_vocab[token]=len(feature_vocab)\n",
    "            if(token in matrix.columns):\n",
    "                feats[feature_vocab[token]]=matrix[token]\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "    vec = TfidfVectorizer(use_idf=False, norm='l1')\n",
    "    all_data = haiku + others\n",
    "    all_data = [\"\".join(x) for x in all_data]\n",
    "    all_data = [x.replace('\\n',' ') for x in all_data]\n",
    "    all_data = [x.replace('\\t',' ') for x in all_data]\n",
    "    all_data = [x.lower() for x in all_data]\n",
    "    matrix = vec.fit_transform(all_data)\n",
    "    matrix = pd.DataFrame(matrix.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=tfIdf_featurize(poem, feature_vocab,matrix)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=tfIdf_featurize(poem, feature_vocab,matrix)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    # we'll use a sparse representation since our features are sparse\n",
    "    X=sparse.lil_matrix((len(data), len(feature_vocab)))\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        for f in feats:\n",
    "            X[idx,f]=feats[f].mean()\n",
    "    return X, Y, feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a06afb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023 NMI\n"
     ]
    }
   ],
   "source": [
    "run_all(haiku, others, fancy_featurize_all_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96459c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.123 NMI\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# approach 2: using BERT sentence transformers\n",
    "def fancy_featurize_all_2(haiku, others):\n",
    "    \n",
    "    def tfIdf_featurize(poem, feature_vocab,matrix):\n",
    "        \n",
    "        # featurize text using BERT Sentence Transformers\n",
    "        feats={}\n",
    "        feats = matrix.encode(poem)\n",
    "        return feats\n",
    "\n",
    "    feature_vocab={}\n",
    "    data=[]\n",
    "    Y=[]\n",
    "    vec = TfidfVectorizer(use_idf=False, norm='l1')\n",
    "    all_data = haiku + others\n",
    "    all_data = [\"\".join(x) for x in all_data]\n",
    "    all_data = [x.replace('\\n',' ') for x in all_data]\n",
    "    all_data = [x.replace('\\t',' ') for x in all_data]\n",
    "    all_data = [x.lower() for x in all_data]\n",
    "    matrix = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "\n",
    "    for poem in haiku:\n",
    "        feats=tfIdf_featurize(poem, feature_vocab,matrix)\n",
    "        data.append(feats)\n",
    "        Y.append(1)\n",
    "    for poem in others:\n",
    "        feats=tfIdf_featurize(poem, feature_vocab,matrix)\n",
    "        data.append(feats)\n",
    "        Y.append(0)\n",
    "    \n",
    "    # since the data above has all haiku ordered before non-haiku, let's shuffle them\n",
    "    temp = list(zip(data, Y))\n",
    "    random.shuffle(temp)\n",
    "    data, Y = zip(*temp)\n",
    "\n",
    "    X = np.zeros((len(data), 768), dtype=object)\n",
    "\n",
    "    for idx,feats in enumerate(data):\n",
    "        X[idx]=feats\n",
    "    return X, Y, feature_vocab\n",
    "\n",
    "run_all(haiku, others, fancy_featurize_all_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feda2b4",
   "metadata": {},
   "source": [
    "**Q2**: Describe your method for featurization in 100 words and why you expect it to be able to separate haiku poems from non-haiku poems in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc992835",
   "metadata": {},
   "source": [
    "The method I have employed is the BERT Sentence Transformer, which seems to work better between the 2 methods I have implemented (TF-IDF and BERT Sentence Transformers). The BERT Sentence Transformer has a bi-directional architecture, i.e., it learns context from left to right, and from right to left, and therefore tends to learn more context than a naive vectorization method such as TF-IDF. What differentiates a haiku from a regular poem (in most cases) is also the imagery and romantic descriptions of nature. Using Sentence Transformers, we are able to learn the context and style of the poem, and are able to differentiate a haiku from a poem more efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64831a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp] *",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
